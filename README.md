
# ğŸ’¬ PDF Chatbot using Local LLM (Ollama + LlamaIndex)

This project allows you to chat with one or more PDF documents **fully offline**, using **Ollama + LlamaIndex**. It supports **text-based** and **image-based (scanned)** PDFs by applying OCR automatically. It builds a searchable index using **local embeddings** and answers your questions in real time.

---

## ğŸ“ Project Structure
```
Project/
â”œâ”€â”€ chatbot.py              # Chatbot interface using LlamaIndex & Ollama
â”œâ”€â”€ ocrExtract.py           # Extracts text from PDFs (OCR fallback)
â”œâ”€â”€ environment.yml         # Conda environment with all dependencies
â”œâ”€â”€ data/                   # Put your PDF files here
â”œâ”€â”€ extracted/pdf_text.txt  # Output of extracted text
â””â”€â”€ index_store/            # Persistent vector index for fast reuse
```

---

## âœ… Features

- ğŸ” Supports both **text** and **scanned image** PDFs  
- ğŸ§  Uses **Ollama local models** like `llama3` or `mistral`  
- ğŸ“¦ Fully offline (no internet required)  
- â™»ï¸ Persistent vector index for fast reloading  
- ğŸ”’ Runs in an isolated conda environment  
- ğŸ“Š Embeddings & LLM both run locally  

---

## âš™ï¸ Setup Instructions

### 1. Install AnacondaÂ orÂ Miniconda  
Download and install from the official site.

### 2. Clone this repo and go to the folder
```bash
git clone <yourâ€‘repoâ€‘url>
cd Project
```

### 3. Create & activate the conda environment
```bash
conda env create -f environment.yml
conda activate myenv
```

### 4. Install & start Ollama
```bash
ollama serve &
ollama pull llama3     # or: ollama pull mistral
```

---

## ğŸ“ Processing PDFs
Put your PDF files inside **`data/`** and run:
```bash
python ocrExtract.py
```
This creates **`extracted/pdf_text.txt`**.

---

## ğŸš€ Start the Chatbot
```bash
python chatbot.py
```

Type questions, e.g.:
```
What penalties are mentioned in the SLA?
```
Type `exit` to quit.

---

## ğŸ”„ Rebuild the Index
```bash
rm -r index_store/
python chatbot.py
```

---

## ğŸ›  Command Cheatâ€‘Sheet
| Purpose                      | Command                                    |
|------------------------------|--------------------------------------------|
| Create env                   | `conda env create -f environment.yml`      |
| Activate env                 | `conda activate myenv`                     |
| Extract PDF text             | `python ocrExtract.py`                     |
| Start chatbot                | `python chatbot.py`                        |
| Pull new Ollama model        | `ollama pull mistral` (or any)             |
| Test model directly          | `ollama run mistral`                       |
| Stop Ollama server           | `pkill -f "ollama"`                        |

---

## ğŸ“¦ `environment.yml` (explained)
```yaml
name: myenv
channels:
  - conda-forge
dependencies:
  - python=3.11          # Base Python version
  - tesseract            # OCR engine backend
  - poppler              # PDF â†’ image conversion tools
  - libtiff              # Image codec dependency
  - pip
  - pip:
      - pytesseract                      # Python wrapper for Tesseract
      - pillow                           # Image handling (PIL)
      - pdf2image                        # Convert PDF pages to images
      - pymupdf                          # Fast PDF text extraction (`fitz`)
      - llama-index>=0.10.34             # Indexing & retrieval library
      - llama-index-llms-ollama          # LlamaIndex â†” Ollama LLM connector
      - llama-index-embeddings-ollama    # Local embedding model via Ollama
      - ollama                           # Python client for Ollama
      - jupyter                          # Optional notebook support
```

---

## âš ï¸ Troubleshooting
- **TypeError None Ã— float** â†’ switch to `mistral` and upgrade `llama-index`.  
- **Timeout on first query** â†’ increase `request_timeout` in `chatbot.py` or preâ€‘warm: `ollama run model`.
